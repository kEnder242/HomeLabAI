WARNING:megatron.core.utils:fused_indices_to_multihot has reached end of life. Please migrate to a non-experimental function.
WARNING:nv_one_logger.api.config:OneLogger: Setting error_handling_strategy to DISABLE_QUIETLY_AND_REPORT_METRIC_ERROR for rank (rank=0) with OneLogger disabled. To override: explicitly set error_handling_strategy parameter.
WARNING:nv_one_logger.training_telemetry.api.training_telemetry_provider:No exporters were provided. This means that no telemetry data will be collected.
2026-02-11 13:23:16,423 - [LAB] INFO - [LOGGER] Reclaim_logger activated (stderr only).
2026-02-11 13:23:16,424 - [LAB] INFO - [BOOT] Mode: SERVICE_UNATTENDED | Door: 8765 (SERVER STARTED)
2026-02-11 13:23:16,424 - [LAB] INFO - [BUILD] Loading Residents (v3.4.19)...
2026-02-11 13:23:20,521 - [ARCHIVE] INFO - NumExpr defaulting to 8 threads.
2026-02-11 13:23:25,109 - [LAB] INFO - [LAB] Archive Connected.
2026-02-11 13:23:27,111 - [LAB] INFO - [LOBBY] Connecting Pinky...
2026-02-11 13:23:27,798 - [LAB] INFO - [LAB] Pinky Connected.
2026-02-11 13:23:29,800 - [LAB] INFO - [LOBBY] Connecting Brain...
2026-02-11 13:23:30,469 - [LAB] INFO - [LAB] Brain Connected.
2026-02-11 13:23:30,469 - [LAB] INFO - [BUILD] Starting EarNode background load...
2026-02-11 13:23:30,469 - [LAB] INFO - [READY] Lab is Open (Lobby Active).
2026-02-11 13:23:30,469 - [LAB] INFO - ðŸ‘‚ EarNode: Attempting to load nvidia/nemotron-speech-streaming-en-0.6b...
[NeMo E 2026-02-11 13:23:55 nemo_logging:417] Failed to load checkpoint with weights_only=True: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 30.12 MiB is free. Process 2842 has 196.55 MiB memory in use. Process 3187 has 159.00 MiB memory in use. Process 4512 has 8.21 MiB memory in use. Process 133643 has 5.41 GiB memory in use. Including non-PyTorch memory, this process has 4.63 GiB memory in use. Of the allocated memory 4.46 GiB is allocated by PyTorch, and 487.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-11 13:23:55,573 - [LAB] ERROR - [EAR_NODE] FATAL: Error during NeMo model loading or CUDA setup: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 30.12 MiB is free. Process 2842 has 196.55 MiB memory in use. Process 3187 has 159.00 MiB memory in use. Process 4512 has 8.21 MiB memory in use. Process 133643 has 5.41 GiB memory in use. Including non-PyTorch memory, this process has 4.63 GiB memory in use. Of the allocated memory 4.46 GiB is allocated by PyTorch, and 487.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/jallred/Dev_Lab/HomeLabAI/src/equipment/ear_node.py", line 49, in __init__
    self.model = EncDecRNNTBPEModel.from_pretrained(model_name=MODEL_NAME)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/classes/common.py", line 885, in from_pretrained
    instance = class_.restore_from(
               ^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/classes/modelPT.py", line 501, in restore_from
    instance = cls._save_restore_connector.restore_from(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 269, in restore_from
    loaded_params = self.load_config_and_state_dict(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 195, in load_config_and_state_dict
    state_dict = self._load_state_dict_from_disk(model_weights, map_location=map_location)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 775, in _load_state_dict_from_disk
    raise e
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 772, in _load_state_dict_from_disk
    return torch.load(model_weights, map_location=map_location, weights_only=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1521, in load
    return _load(
           ^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 2122, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/_weights_only_unpickler.py", line 535, in load
    self.append(self.persistent_load(pid))
                ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 2086, in persistent_load
    typed_storage = load_tensor(
                    ^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 2052, in load_tensor
    wrap_storage = restore_location(storage, location)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1864, in restore_location
    return default_restore_location(storage, str(map_location))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 698, in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 637, in _deserialize
    return obj.to(device=device)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/storage.py", line 291, in to
    return _to(self, device, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/_utils.py", line 101, in _to
    untyped_storage = torch.UntypedStorage(self.size(), device=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 30.12 MiB is free. Process 2842 has 196.55 MiB memory in use. Process 3187 has 159.00 MiB memory in use. Process 4512 has 8.21 MiB memory in use. Process 133643 has 5.41 GiB memory in use. Including non-PyTorch memory, this process has 4.63 GiB memory in use. Of the allocated memory 4.46 GiB is allocated by PyTorch, and 487.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-11 13:23:55,577 - [LAB] WARNING - [EAR_NODE] Attempting fallback: Re-initializing with CUDA graphs explicitly disabled.
2026-02-11 13:24:05,527 - [LAB] ERROR - [EAR_NODE] FATAL: Fallback failed: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 12.12 MiB is free. Process 2842 has 196.55 MiB memory in use. Process 3187 has 159.00 MiB memory in use. Process 4512 has 8.21 MiB memory in use. Process 133643 has 5.41 GiB memory in use. Including non-PyTorch memory, this process has 4.65 GiB memory in use. Of the allocated memory 4.48 GiB is allocated by PyTorch, and 66.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/jallred/Dev_Lab/HomeLabAI/src/equipment/ear_node.py", line 49, in __init__
    self.model = EncDecRNNTBPEModel.from_pretrained(model_name=MODEL_NAME)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/classes/common.py", line 885, in from_pretrained
    instance = class_.restore_from(
               ^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/classes/modelPT.py", line 501, in restore_from
    instance = cls._save_restore_connector.restore_from(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 269, in restore_from
    loaded_params = self.load_config_and_state_dict(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 195, in load_config_and_state_dict
    state_dict = self._load_state_dict_from_disk(model_weights, map_location=map_location)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 775, in _load_state_dict_from_disk
    raise e
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 772, in _load_state_dict_from_disk
    return torch.load(model_weights, map_location=map_location, weights_only=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1521, in load
    return _load(
           ^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 2122, in _load
    result = unpickler.load()
             ^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/_weights_only_unpickler.py", line 535, in load
    self.append(self.persistent_load(pid))
                ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 2086, in persistent_load
    typed_storage = load_tensor(
                    ^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 2052, in load_tensor
    wrap_storage = restore_location(storage, location)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 1864, in restore_location
    return default_restore_location(storage, str(map_location))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 698, in default_restore_location
    result = fn(storage, location)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/serialization.py", line 637, in _deserialize
    return obj.to(device=device)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/storage.py", line 291, in to
    return _to(self, device, non_blocking)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/_utils.py", line 101, in _to
    untyped_storage = torch.UntypedStorage(self.size(), device=device)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 30.12 MiB is free. Process 2842 has 196.55 MiB memory in use. Process 3187 has 159.00 MiB memory in use. Process 4512 has 8.21 MiB memory in use. Process 133643 has 5.41 GiB memory in use. Including non-PyTorch memory, this process has 4.63 GiB memory in use. Of the allocated memory 4.46 GiB is allocated by PyTorch, and 487.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jallred/Dev_Lab/HomeLabAI/src/equipment/ear_node.py", line 73, in __init__
    self.model = EncDecRNNTBPEModel.from_pretrained(model_name=MODEL_NAME)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/classes/common.py", line 885, in from_pretrained
    instance = class_.restore_from(
               ^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/classes/modelPT.py", line 501, in restore_from
    instance = cls._save_restore_connector.restore_from(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 269, in restore_from
    loaded_params = self.load_config_and_state_dict(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/nemo/core/connectors/save_restore_connector.py", line 191, in load_config_and_state_dict
    instance = instance.to(map_location)
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/lightning/fabric/utilities/device_dtype_mixin.py", line 55, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1371, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 930, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 957, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/home/jallred/Dev_Lab/HomeLabAI/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1357, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 12.12 MiB is free. Process 2842 has 196.55 MiB memory in use. Process 3187 has 159.00 MiB memory in use. Process 4512 has 8.21 MiB memory in use. Process 133643 has 5.41 GiB memory in use. Including non-PyTorch memory, this process has 4.65 GiB memory in use. Of the allocated memory 4.48 GiB is allocated by PyTorch, and 66.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
2026-02-11 13:24:05,531 - [LAB] ERROR - [STT] Load Failed: EarNode failed to load even with fallback: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 30.12 MiB is free. Process 2842 has 196.55 MiB memory in use. Process 3187 has 159.00 MiB memory in use. Process 4512 has 8.21 MiB memory in use. Process 133643 has 5.41 GiB memory in use. Including non-PyTorch memory, this process has 4.63 GiB memory in use. Of the allocated memory 4.46 GiB is allocated by PyTorch, and 487.50 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) / CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 10.57 GiB of which 12.12 MiB is free. Process 2842 has 196.55 MiB memory in use. Process 3187 has 159.00 MiB memory in use. Process 4512 has 8.21 MiB memory in use. Process 133643 has 5.41 GiB memory in use. Including non-PyTorch memory, this process has 4.65 GiB memory in use. Of the allocated memory 4.48 GiB is allocated by PyTorch, and 66.00 KiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
